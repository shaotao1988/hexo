

传统的数据库只能保证单个数据库数据修改的事务性，而在分布式系统中，为保证数据高可用和系统吞吐量，数据通常被分片和复制，存储在不同的物理机器上的不同数据库中，如何同步修改位于不同物理机器上的数据，是一个严峻的挑战。

解决分布式一致性问题比较著名的协议有2阶段提交(2PC, 2 Phase Commit)，3阶段提交(3PC，3 Phase Commit)和Paxo协议。

# 2PC

2阶段提交和3阶段提交都引入了事务协调者(XA Manager)的角色，对参与分布式事务的节点(XA Resource)的提交过程进行管理，保证事务的原子性(所有节点要么全做，要么全不做)。

1. 第一阶段：提交请求(vote)
   
   ![2PC vote](2PC_vote.jpg)

   XA Manager向所有的XA Resource发起提交请求，XA Resource收到请求后获取相关资源，**执行事务操作**，并根据处理结果进行投票，向XA Manager发送OK或Abort应答。

2. 第二阶段：提交
   
   ![2PC commit](2PC_commit.jpg)

   XA Manager收到所有XA Resource的应答后，进行第二阶段的操作。如果所有XA Resource都返回OK，则向所有XA Resource发送提交命令，否则发送回滚命令。XA Resource收到提交或回滚命令后完成提交或回滚并释放整个事务期间占用的资源。XA Manager收到所有XA Resource的提交完成或回滚完成的应答后，结束或取消事务。

2阶段提交的回滚是通过记录undo/redo日志来实现的。

2阶段提交看起来确实能提供原子性操作，但是仍有一些缺点：

- 阻塞
  2阶段提交是阻塞式协议，XA Resource在第一阶段应答完成后，将一直处于阻塞状态，直到收到第二阶段的提交或回滚命令，该状态下一直占用锁资源，也会导致其他需要获取这个锁的进程处于阻塞状态。
- 数据不一致
  在第二阶段执行提交的过程中，如果XA Manager发送提交命令后宕机或者网络故障，导致只有一部分XA Resource接收到提交命令，那么就会有一部分XA Resource无法执行提交，整个分布式系统就出现了数据不一致的现象。
  另外，如果XA Manager发送提交命令后宕机，并且唯一接收到提交命令的XA Resource也宕机，那么即使通过选举产生了新的协调者，这条事务的状态也是不确定的：新的协调者无法确定已经宕机XA Resource是否已经完成提交，如果新的协调者发起提交，但是宕机的XA Resource并没有提交，数据就不一致了，或者如果新的协调者发起回滚，但是宕机的XA Resource已经提交成功了，数据也会不一致。
- 单点故障
  由于XA Manager在整个协议的运行中发挥不可替代的作用，一旦其发生网络故障或宕机，会导致整个协议处于挂死状态，可以通过定时器，或重新选举新的manager来接管协议状态，但是仍然可能存在第二点中数据不一致的问题

# 3PC

在2阶段提交协议中，所有XA Resource节点收到第二阶段的提交命令后，直接提交，而不管其他节点是否能提交成功。这样做的问题就是，如果唯一一个收到提交命令的Resource节点和Manager节点同时宕机，那么无法判断这个事务是否提交成功：如果宕机的节点已经提交成功，那么协议简单的让所有其他节点回滚，数据就不一致了；反之如果协议简单地让其他节点都成功提交，若宕机的节点并未提交成功，数据也会不一致，因此协议就陷入了两难的境地。

3PC在一定程度上规避了这个问题，引入超时机制，以及在第二阶段发送提交命令前再加一个阶段：准备提交(prepare to commit)。当manager收到第一阶段的投票结果并确认结果为提交时，向所有节点发送准备提交命令。节点收到准备提交命令后，进入中间状态，在该状态下能够随时进行提交（该阶段不能做无法undo的事情），然后向manager发送成功响应。

添加这个步骤的作用是，所有参与事务的节点进行提交前最后的沟通，当这个步骤成功完成以后，无论哪个非manager节点宕机，都不影响协议的恢复。

当Manager节点宕机后，协议将进入恢复模式。在该模式下会产生新的Manager节点，并向任意其它在线的Resource节点查询当前状态。如果已经处于prepare to commit完成状态，那么manager就可以安全的执行第三步提交指令了（如果节点处于prepare to commit状态后一直未收到第三步的指令，超时后默认提交）；如果Resource节点还没到prepare to commit状态，manager则可以确定这个事务还没有提交，可以安全的进行回滚或重试。

在某些特定场景下，三阶段提交协议仍然不能保证数据的一致性。比如在网络分区场景下，所有分区一中的节点都收到prepare to commit指令，而分区二中的所有节点都没有收到第二阶段的准备提交指令，当这2个网络分区各自进入恢复模式后，协议在2个分区的运行结果分别是提交和回滚，网络恢复后数据又处于不一致的状态。

三阶段提交协议的优点在于，任何节点宕机后协议都不会被block。

# Paxos

3PC的容错机制是为fail-stop模型设计的：机器宕机后永远无法恢复。而很多时候，错误大部分是fail-recover模型：因为网络拥塞、或者CPU占用高，导致Manager节点一段时间无法响应，进而导致协议超时。如果所有节点都像Manager节点发送了prepare to commit成功的应答，并在收到提交命令前Manager发生网络故障，协议超时后重新选举了新的Manger，查询其他节点状态得知均处于提交准备完成状态，于是向所有节点发起提交。同时，原来的Manager从网络故障、或CPU忙中恢复，由于协议超时且只收到部分节点的prepare to commit的确认应答，会向所有节点发送回滚命令。如果这2个时间点刚好处于同一时刻，那么哪个消息先到达节点是不确定的，可能导致有些节点提交而另一些节点回滚，协议的运行结果又一次处于不确定的状态。

## Paxos协议的三种角色

- Proposer

    协议的发起者，只有被Proposer提交过的提议才能被最终选中，没有提议提出，协议就不会有结果

- Acceptor

    所有对Proposer提议的值具有投票权的节点。Acceptor要么拒绝提议，要么同意提议，并返回一个承诺。承诺的内容和作用后面详述。

- Listener

    只要大多数Acceptor就相同的提议达成一致，那么Paxos协议就可以终止并返回提议值，该提议值会传播到所有对提议感兴趣的节点，这些节点就叫Listener。

## Paxos协议流程

就框架而言，Paxos和2PC非常像。Proposer向Acceptors发送'prepare'请求，当Acceptors接受该请求并发送响应后，Proposer向Acceptors发送提交请求，最后Acceptors向Proposer返回提交的结果。一旦足够数量的Acceptors提交成功并通知了Proposer，协议终止。Paxos的魔法在于Acceptor何时可以接受一个‘prepare'请求，以及Acceptor如何选择正确的提议值。

相比于2PC，Paxos增加了2项重要机制。第一是对提议进行编号，以区别不同的提议，以及供Acceptors识别提议到达的先后顺序。第二是只要大多数Acceptors接受提议，就可以同意这个提议，所以只要不超过一半的节点失效，Paxos协议仍然可以正常工作。

提议(Proposal) = 提议的值(Proposal Value) + 提议编号(Proposal Number)，二者缺一不可。

记{n, v}为提议编号为n，提议值为v的提议，记(m, {n, v})为承诺了Prepare(m)请求，并接受了提议{n, v}。

### 提议编号

Proposer提的每个提议都必须包含全局唯一的提议编号，供Acceptors识别提议到达的先后顺序。当提议到达时，Acceptor首先检查之前已经收到过的提议的最大编号。如果新的提议编号大于这个值，Acceptor通过该提议并返回一个承诺，保证以后不会通过任何提议编号小于它的提议；如果新的提议编号小于这个值，Acceptor将会拒绝该提议，并返回当前接收到的最大提议编号，以提示该Proposer下次发起新提议时必须选择一个更大的值作为提议编号。

提议编号机制保证了无论提议以何种顺序到达Acceptor，Acceptor都能在不与Proposer进行进一步通讯的情况下，决策通过哪一项提议。这解决了信息异步的问题-不同节点上的提议可能以不同的顺序到达Acceptors。

随之而来的问题就是，如何保证不同Proposer发起提议的提议编号是全局唯一的？最简单的方法是让所有的Proposer在互不相交的数字集合中选择提议编号。一个典型的做法是选择(seq.number, address)作为提议编号，其中address是该Proposer的全局唯一IP地址。这个数字对是全局唯一的，并且对每一个Proposer来说，只要选择的seq.number足够大，就有机会胜出。

### 大多数

